import OpenAI from 'openai';
import {LLMProvider, ProviderResponseChunk} from "./LLMProvider";
import {type ChatCompletionMessageParam} from "openai/resources";
import {type ChatMessage, ProviderConfig} from "../../../../share/type";

// import { Ollama } from "ollama";

// const ollama = new Ollama();

export class OpenAIProvider implements LLMProvider {
	private openai: OpenAI;
	private config: ProviderConfig;

	constructor(config: ProviderConfig) {
		this.config = config;

		this.openai = new OpenAI({
			apiKey: this.config.apiKey,
			baseURL: this.config.apiUrl
		});
	}

	formatMessage(messages: ChatMessage[]): Array<ChatCompletionMessageParam> {
		return messages.map((message) => {
			// å¤„ç†åŒ…å«å›¾ç‰‡çš„æ¶ˆæ¯
			if (message.images && message.images.length > 0) {
				const content: any[] = [];
				
				// å¦‚æžœæœ‰æ–‡æœ¬å†…å®¹ï¼Œæ·»åŠ æ–‡æœ¬éƒ¨åˆ†
				if (typeof message.content === 'string') {
					content.push({
						type: 'text',
						text: message.content
					});
				}
				
				// æ·»åŠ æ‰€æœ‰å›¾ç‰‡
				message.images.forEach(image => {
					let imageUrl = '';
					if (typeof image === 'string') {
						imageUrl = image;
					} else if (image instanceof Uint8Array) {
						// å¤„ç†äºŒè¿›åˆ¶å›¾ç‰‡æ•°æ® - è¿™é‡Œå¯èƒ½éœ€è¦è½¬æ¢ä¸º base64 æˆ–å…¶ä»–æ ¼å¼
						// è¿™éƒ¨åˆ†éœ€è¦æ ¹æ®å®žé™…éœ€æ±‚å®žçŽ°
					} else if ('url' in image) {
						imageUrl = image.url || '';
					} else if ('path' in image) {
						imageUrl = image.path || '';
					}
					
					if (imageUrl) {
						content.push({
							type: 'image_url',
							image_url: {
								url: imageUrl
							}
						});
					}
				});
				
				return {
					role: message.role,
					content: content
				};
			}
			
			// å¤„ç†åŒ…å«æ–‡ä»¶çš„æ¶ˆæ¯
			if (message.files) {
				// è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦å¤„ç†æ–‡ä»¶
				// ç›®å‰ OpenAI API å¯èƒ½ä¸ç›´æŽ¥æ”¯æŒæ–‡ä»¶é™„ä»¶ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
				// ä¾‹å¦‚å°†æ–‡ä»¶å†…å®¹ä½œä¸ºæ–‡æœ¬æ·»åŠ ï¼Œæˆ–è€…ä¸Šä¼ æ–‡ä»¶åŽèŽ·å– URL
			}
			
			// é»˜è®¤å¤„ç†çº¯æ–‡æœ¬æ¶ˆæ¯
			return {
				role: message.role,
				content: message.content
			};
		});
	}

	async chat(messages: ChatMessage[], modelId: string, signal?: AbortSignal) {
		try {
			const msgs = messages.map((msg) => {
				return {
					role: msg.role,
					content: msg.content
				}
			}) as  ChatCompletionMessageParam[]
			// https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2712576.html
			const response = await this.openai.chat.completions.create({
				model: modelId,
				messages: msgs,
				temperature: this.config.temperature,
				max_completion_tokens: this.config.maxTokens,
				stream: false,
			}, {
				signal
			});

			return response.choices[0].message;
		} catch (error) {
			console.error('OpenAI API Error:', error);
			throw new Error('Failed to get response from OpenAI');
		}
	}

	async* streamChat(messages: ChatMessage[], modelId: string, signal?: AbortSignal) {
		// as ChatCompletionMessageParam[]
		const msgs  = messages.map((msg) => {
			return {
				role: msg.role,
				content: msg.content,
			}
		}) as ChatCompletionMessageParam[];
		try {
			console.log('ðŸ“¦ OpenAI Stream:', msgs);
			const stream = await this.openai.chat.completions.create({
				model: modelId,
				messages: msgs,
				temperature: this.config.temperature,
				max_tokens: this.config.maxTokens,
				stream: true,
			}, {
				signal
			});
			for await (const part of stream) {
				let content = part.choices[0]?.delta?.content;
				yield {
					id: part.id,
					content: content,
					provider: 'openai',
					model: modelId,
					role: part.choices[0].delta.role,
					timestamp: part.created,
					type: 'text'
				} as ProviderResponseChunk;
			}
		} catch (error) {
			console.error('OpenAI API Stream Error:', error);
			throw new Error('Failed to create stream from OpenAI');
		}
	}

	async getModels() {
		try {
			// const tags = await ollama.tags();
			console.log('ðŸ“¦ Local model list:');
			// return [];
			return this.openai.models.list()
		} catch (error) {
			console.error('Ollama API Error:', error);
			throw new Error('Failed to get models from Ollama');
		}
	}
}


// async function test(){
//   try {
//
//     const open = new OpenAIProvider({} as ProviderConfig);
//     const res = await open.chat([{role: 'user', content: 'hello'}])
//     console.log(res, 333);
//   }catch (e) {
//     console.log(e);
//   }
//
// }
//
// test();
